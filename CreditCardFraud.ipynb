{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CreditCardFraud",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tinosai/tinosai.github.io/blob/master/CreditCardFraud.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56v22CrqA10i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "accd457a-b013-47f4-eeca-be7cc16db728"
      },
      "source": [
        "import os\n",
        "!echo '{\"username\":\"tino91\",\"key\":\"fc01868fec179ecbb5efeeef5522ca43\"}'>kaggle.json\n",
        "!mv kaggle.json /root/.kaggle/\n",
        "!kaggle datasets download -d mlg-ulb/creditcardfraud\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading creditcardfraud.zip to /content\n",
            " 92% 61.0M/66.0M [00:00<00:00, 47.1MB/s]\n",
            "100% 66.0M/66.0M [00:00<00:00, 79.5MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXJEOdSRA3e9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "3a1817b5-d87c-4101-8c9e-d1c87b44b364"
      },
      "source": [
        "!unzip creditcardfraud.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  creditcardfraud.zip\n",
            "  inflating: creditcard.csv          \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaJ9iFgFCD1-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7b302a2b-312e-446b-b877-857721ef3755"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# In[1]:\n",
        "\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# In[2]:\n",
        "\n",
        "\n",
        "df=pd.read_csv('creditcard.csv')\n",
        "\n",
        "\n",
        "# In[3]:\n",
        "\n",
        "\n",
        "df.head()\n",
        "\n",
        "\n",
        "# In[4]:\n",
        "\n",
        "\n",
        "#Time is not needed\n",
        "df=df.drop([\"Time\"],axis=1)\n",
        "df.head()\n",
        "\n",
        "\n",
        "# In[5]:\n",
        "\n",
        "\n",
        "#since the great majority of the transactions are non fraudulent, let's look for the first 10 fraudulent transactions\n",
        "df[df[\"Class\"]==1].head(10).drop([\"Class\"],axis=1)\n",
        "\n",
        "\n",
        "# In[73]:\n",
        "\n",
        "\n",
        "#let's reshuffle the dataset\n",
        "df=df.sample(frac=1)\n",
        "\n",
        "\n",
        "# In[74]:\n",
        "\n",
        "\n",
        "#lets create train, validation, test sets\n",
        "p=0.8\n",
        "kiri=int(p*df.shape[0])\n",
        "train_X,val_X,test_X=np.array(df.iloc[:kiri,:-1]),np.array(df.iloc[kiri::2,:-1]),np.array(df.iloc[kiri+1::2,:-1])\n",
        "train_Y,val_Y,test_Y=np.array(df.iloc[:kiri,-1]),np.array(df.iloc[kiri::2,-1]),np.array(df.iloc[kiri+1::2,-1])\n",
        "\n",
        "\n",
        "# In[77]:\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "train_Y_count=Counter(train_Y)\n",
        "val_Y_count=Counter(val_Y)\n",
        "test_Y_count=Counter(test_Y)\n",
        "\n",
        "\n",
        "# In[78]:\n",
        "\n",
        "\n",
        "print(\"In train set: {} negative and {} positive\".format(train_Y_count[0],train_Y_count[1]))\n",
        "print(\"In val set: {} negative and {} positive\".format(val_Y_count[0],val_Y_count[1]))\n",
        "print(\"In test set: {} negative and {} positive\".format(test_Y_count[0],test_Y_count[1]))\n",
        "\n",
        "print(\"Train Set, Positive/Negative: {} %\".format(train_Y_count[1]/train_Y_count[0]*100))\n",
        "print(\"Val Set, Positive/Negative: {} %\".format(val_Y_count[1]/val_Y_count[0]*100))\n",
        "print(\"Test Set, Positive/Negative: {} %\".format(test_Y_count[1]/test_Y_count[0]*100))\n",
        "\n",
        "\n",
        "# The percentage of positives in the validation and test set is higher than in the training set. As a result, this would guarantee more stringent requirements on accuracy.\n",
        "\n",
        "# In[255]:\n",
        "\n",
        "\n",
        "#first of all, we need to normalize the inputs\n",
        "#mean vector\n",
        "mean_vector=train_X.mean(axis=0)\n",
        "#std vector\n",
        "std_vector=train_X.std(axis=0)\n",
        "\n",
        "\n",
        "# In[256]:\n",
        "\n",
        "\n",
        "normalized_train_X,normalized_val_X,normalized_test_X=(train_X-mean_vector)/std_vector,(val_X-mean_vector)/std_vector,(test_X-mean_vector)/std_vector\n",
        "\n",
        "\n",
        "# In[526]:\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "# In[527]:\n",
        "\n",
        "\n",
        "#TensorDataset creation\n",
        "train_data=TensorDataset(torch.from_numpy(normalized_train_X),torch.from_numpy(train_Y))\n",
        "val_data=TensorDataset(torch.from_numpy(normalized_val_X),torch.from_numpy(val_Y))\n",
        "test_data=TensorDataset(torch.from_numpy(normalized_test_X),torch.from_numpy(test_Y))\n",
        "\n",
        "\n",
        "# In[528]:\n",
        "\n",
        "\n",
        "#class imbalance treatment\n",
        "num_classes=[train_Y_count[0],train_Y_count[1]]\n",
        "weights=1./torch.tensor(num_classes,dtype=float)\n",
        "sample_weights=weights[train_Y]\n",
        "sampler=torch.utils.data.WeightedRandomSampler(weights=sample_weights,num_samples=len(sample_weights),replacement=True)\n",
        "\n",
        "\n",
        "# In[771]:\n",
        "\n",
        "\n",
        "#creating the loaders\n",
        "batch_size=50001 #was 100\n",
        "train_loader=DataLoader(train_data,sampler=sampler,batch_size=batch_size)\n",
        "val_loader=DataLoader(val_data,shuffle=True,batch_size=batch_size)\n",
        "test_loader=DataLoader(test_data,shuffle=False,batch_size=batch_size)\n",
        "\n",
        "\n",
        "# In[910]:\n",
        "\n",
        "\n",
        "train_iter=iter(val_loader)\n",
        "inputs,outputs=train_iter.next()\n",
        "print(inputs)\n",
        "\n",
        "\n",
        "# In[983]:\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# In[984]:\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net,self).__init__()\n",
        "        self.fc1=nn.Linear(29,20)\n",
        "        self.fc2=nn.Linear(20,10)\n",
        "        self.fc3=nn.Linear(10,3)\n",
        "        self.fc4=nn.Linear(3,1)\n",
        "        #self.fc1=nn.Linear(29,15)\n",
        "        #self.fc2=nn.Linear(15,8)\n",
        "        #self.fc3=nn.Linear(8,3)\n",
        "        #self.fc4=nn.Linear(3,1)\n",
        "        \n",
        "        \n",
        "    def forward(self,x):\n",
        "        x=F.relu(self.fc1(x))\n",
        "        x=F.relu(self.fc2(x))\n",
        "        x=F.relu(self.fc3(x))\n",
        "        x=self.fc4(x)        \n",
        "        return x\n",
        "\n",
        "\n",
        "# In[985]:\n",
        "\n",
        "\n",
        "model=Net()\n",
        "\n",
        "\n",
        "# In[986]:\n",
        "\n",
        "\n",
        "model\n",
        "\n",
        "\n",
        "# In[987]:\n",
        "\n",
        "\n",
        "criterion=nn.BCEWithLogitsLoss()\n",
        "optimizer=torch.optim.Adam(model.parameters(),lr=0.001,weight_decay=1e-4)\n",
        "#0.0005, 1e-3, F1=0.485 @50 iter\n",
        "#0.0001, 1e-2, F1=0.205 @50iter \n",
        "#0.00025, 1e-3, F1=0.3786 @50iter\n",
        "#0.0003, 5e-4, F1=0.52 @50iter\n",
        "#0.0003, 2.5e-4, F1=0.5970 @50iter\n",
        "#0.00025,2.5e-4, F1=0.457 @50iter\n",
        "#0.00035,2.5e-4, F1=0.5674 @50iter\n",
        "#0.0003,2e-4, F1=0.5672 @50iter\n",
        "#0.0003, 3.0e-4, F1=0.5909 @50iter\n",
        "#0.00035, 2.5e-4, F1= @50iter\n",
        "\n",
        "#0.00015, 2.5e-4, F1=0.644 @500iter\n",
        "#0.00015, 1e-4, F1=0.7723 @500iter\n",
        "#0.00015, 0.75e-4, F1=0.7047 @500iter\n",
        "#0.0001, 1e-4, F1=0.7358 @500iter\n",
        "#0.00015, 1e-4, F1=0.65117 @50iter #optimizer beta 0.8\n",
        "#0.00015, 3e-4, F1=0.6190 @50iter #optimizer beta 0.8\n",
        "#0.00015, 1e-4, F1=0.6722 @50iter\n",
        "#0.0005, 1e-4, F1= @50iter\n",
        "\n",
        "\n",
        "# In[988]:\n",
        "\n",
        "\n",
        "print(optimizer)\n",
        "\n",
        "\n",
        "# In[989]:\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "def train(n_epochs,loaders,model,optimizer,criterion):\n",
        "    list_train_loss=[]\n",
        "    list_valid_loss=[]\n",
        "    for epoch in range(1,n_epochs+1):\n",
        "        train_loss=0.0\n",
        "        valid_loss=0.0\n",
        "        model.train()\n",
        "        for batch_idx,(data,target) in enumerate(loaders[\"train\"]):\n",
        "            optimizer.zero_grad()\n",
        "            data,target=data.cuda(),target.cuda()\n",
        "            output=model(data.float())\n",
        "            loss=criterion(output.squeeze(),target.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss=train_loss+1/(batch_idx+1)*(loss.item()-train_loss)\n",
        "        list_train_loss.append(train_loss)\n",
        "        print(\"At {} epoch, Training Loss: {} \".format(epoch,train_loss))\n",
        "        model.eval()\n",
        "        for batch_idx,(data,target) in enumerate(loaders[\"valid\"]):\n",
        "            data,target=data.cuda(),target.cuda()\n",
        "            output=model(data.float())\n",
        "            loss=criterion(output.squeeze(),target.float())\n",
        "            valid_loss=valid_loss+1/(batch_idx+1)*(loss.item()-valid_loss)\n",
        "        list_valid_loss.append(valid_loss)\n",
        "        print(\"At {} epoch, Validation Loss: {} \".format(epoch,valid_loss))\n",
        "    return list_train_loss, list_valid_loss\n",
        "\n",
        "\n",
        "# In[990]:\n",
        "\n",
        "\n",
        "#loader wrapping\n",
        "loaders={'train':train_loader, 'valid':val_loader}\n",
        "\n",
        "\n",
        "# In[991]:\n",
        "\n",
        "\n",
        "train_loss,valid_loss=train(150,loaders,model,optimizer,criterion) \n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "plt.plot(train_loss)\n",
        "plt.plot(valid_loss)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In train set: 227451 negative and 394 positive\n",
            "In val set: 28438 negative and 43 positive\n",
            "In test set: 28426 negative and 55 positive\n",
            "Train Set, Positive/Negative: 0.17322412299792042 %\n",
            "Val Set, Positive/Negative: 0.15120613263942612 %\n",
            "Test Set, Positive/Negative: 0.19348483782452686 %\n",
            "tensor([[ 0.8085, -0.8916, -0.4972,  ..., -0.2322, -0.0387,  0.6520],\n",
            "        [-1.4035, -1.7850,  1.6955,  ...,  0.4594, -1.1748,  0.3357],\n",
            "        [-0.3642, -0.2338,  0.9832,  ...,  0.8763,  0.3724, -0.3571],\n",
            "        ...,\n",
            "        [ 0.5733, -0.5754,  0.2896,  ..., -0.0249,  0.1054,  0.2208],\n",
            "        [-0.2265,  0.3950,  0.6745,  ...,  0.9162,  0.5195, -0.3495],\n",
            "        [-0.3750,  1.1196, -0.8985,  ...,  0.6489,  0.3527, -0.3314]],\n",
            "       dtype=torch.float64)\n",
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0.0001\n",
            ")\n",
            "At 1 epoch, Training Loss: 0.6910328388214111 \n",
            "At 1 epoch, Validation Loss: 0.9237477779388428 \n",
            "At 2 epoch, Training Loss: 0.6753760576248169 \n",
            "At 2 epoch, Validation Loss: 0.9199360609054565 \n",
            "At 3 epoch, Training Loss: 0.6572829723358155 \n",
            "At 3 epoch, Validation Loss: 0.9169966578483582 \n",
            "At 4 epoch, Training Loss: 0.6341595888137818 \n",
            "At 4 epoch, Validation Loss: 0.914759635925293 \n",
            "At 5 epoch, Training Loss: 0.610467278957367 \n",
            "At 5 epoch, Validation Loss: 0.9124812483787537 \n",
            "At 6 epoch, Training Loss: 0.5861703038215638 \n",
            "At 6 epoch, Validation Loss: 0.9090756773948669 \n",
            "At 7 epoch, Training Loss: 0.566347873210907 \n",
            "At 7 epoch, Validation Loss: 0.9036917090415955 \n",
            "At 8 epoch, Training Loss: 0.5447447061538696 \n",
            "At 8 epoch, Validation Loss: 0.8962889313697815 \n",
            "At 9 epoch, Training Loss: 0.5280266880989075 \n",
            "At 9 epoch, Validation Loss: 0.8871647119522095 \n",
            "At 10 epoch, Training Loss: 0.5149923205375672 \n",
            "At 10 epoch, Validation Loss: 0.8770557641983032 \n",
            "At 11 epoch, Training Loss: 0.5046944379806518 \n",
            "At 11 epoch, Validation Loss: 0.8665527701377869 \n",
            "At 12 epoch, Training Loss: 0.4953292667865753 \n",
            "At 12 epoch, Validation Loss: 0.8561322093009949 \n",
            "At 13 epoch, Training Loss: 0.4852632224559784 \n",
            "At 13 epoch, Validation Loss: 0.8460782170295715 \n",
            "At 14 epoch, Training Loss: 0.47799370884895326 \n",
            "At 14 epoch, Validation Loss: 0.836449921131134 \n",
            "At 15 epoch, Training Loss: 0.47162901759147646 \n",
            "At 15 epoch, Validation Loss: 0.8273341655731201 \n",
            "At 16 epoch, Training Loss: 0.46533082127571107 \n",
            "At 16 epoch, Validation Loss: 0.8185606598854065 \n",
            "At 17 epoch, Training Loss: 0.4609382688999176 \n",
            "At 17 epoch, Validation Loss: 0.8095588088035583 \n",
            "At 18 epoch, Training Loss: 0.45398114919662474 \n",
            "At 18 epoch, Validation Loss: 0.7999712228775024 \n",
            "At 19 epoch, Training Loss: 0.4506567656993866 \n",
            "At 19 epoch, Validation Loss: 0.7895182967185974 \n",
            "At 20 epoch, Training Loss: 0.44190938472747804 \n",
            "At 20 epoch, Validation Loss: 0.7779091000556946 \n",
            "At 21 epoch, Training Loss: 0.43725786209106443 \n",
            "At 21 epoch, Validation Loss: 0.764568567276001 \n",
            "At 22 epoch, Training Loss: 0.4283872485160828 \n",
            "At 22 epoch, Validation Loss: 0.7492013573646545 \n",
            "At 23 epoch, Training Loss: 0.4213995456695557 \n",
            "At 23 epoch, Validation Loss: 0.731601357460022 \n",
            "At 24 epoch, Training Loss: 0.41258575916290285 \n",
            "At 24 epoch, Validation Loss: 0.71144700050354 \n",
            "At 25 epoch, Training Loss: 0.40267894268035886 \n",
            "At 25 epoch, Validation Loss: 0.6891418099403381 \n",
            "At 26 epoch, Training Loss: 0.39315603375434877 \n",
            "At 26 epoch, Validation Loss: 0.664922833442688 \n",
            "At 27 epoch, Training Loss: 0.3800180971622467 \n",
            "At 27 epoch, Validation Loss: 0.638761043548584 \n",
            "At 28 epoch, Training Loss: 0.3666608989238739 \n",
            "At 28 epoch, Validation Loss: 0.6108710765838623 \n",
            "At 29 epoch, Training Loss: 0.3531879663467407 \n",
            "At 29 epoch, Validation Loss: 0.5811999440193176 \n",
            "At 30 epoch, Training Loss: 0.33867424726486206 \n",
            "At 30 epoch, Validation Loss: 0.549801230430603 \n",
            "At 31 epoch, Training Loss: 0.3239901542663574 \n",
            "At 31 epoch, Validation Loss: 0.5163398385047913 \n",
            "At 32 epoch, Training Loss: 0.30657978653907775 \n",
            "At 32 epoch, Validation Loss: 0.48119738698005676 \n",
            "At 33 epoch, Training Loss: 0.2909103274345398 \n",
            "At 33 epoch, Validation Loss: 0.44513702392578125 \n",
            "At 34 epoch, Training Loss: 0.2746498942375183 \n",
            "At 34 epoch, Validation Loss: 0.4088578522205353 \n",
            "At 35 epoch, Training Loss: 0.25695949196815493 \n",
            "At 35 epoch, Validation Loss: 0.37334567308425903 \n",
            "At 36 epoch, Training Loss: 0.24134850203990937 \n",
            "At 36 epoch, Validation Loss: 0.33947044610977173 \n",
            "At 37 epoch, Training Loss: 0.22540906369686126 \n",
            "At 37 epoch, Validation Loss: 0.30733197927474976 \n",
            "At 38 epoch, Training Loss: 0.20841506421566008 \n",
            "At 38 epoch, Validation Loss: 0.27711519598960876 \n",
            "At 39 epoch, Training Loss: 0.19645726978778838 \n",
            "At 39 epoch, Validation Loss: 0.2498684674501419 \n",
            "At 40 epoch, Training Loss: 0.18409112393856047 \n",
            "At 40 epoch, Validation Loss: 0.22605010867118835 \n",
            "At 41 epoch, Training Loss: 0.17426787912845612 \n",
            "At 41 epoch, Validation Loss: 0.20628604292869568 \n",
            "At 42 epoch, Training Loss: 0.16384150087833405 \n",
            "At 42 epoch, Validation Loss: 0.18981003761291504 \n",
            "At 43 epoch, Training Loss: 0.15628116130828856 \n",
            "At 43 epoch, Validation Loss: 0.17480909824371338 \n",
            "At 44 epoch, Training Loss: 0.14877721071243286 \n",
            "At 44 epoch, Validation Loss: 0.16096720099449158 \n",
            "At 45 epoch, Training Loss: 0.14292859137058259 \n",
            "At 45 epoch, Validation Loss: 0.15029069781303406 \n",
            "At 46 epoch, Training Loss: 0.1367048978805542 \n",
            "At 46 epoch, Validation Loss: 0.14064446091651917 \n",
            "At 47 epoch, Training Loss: 0.13070813715457916 \n",
            "At 47 epoch, Validation Loss: 0.13177889585494995 \n",
            "At 48 epoch, Training Loss: 0.12737911343574523 \n",
            "At 48 epoch, Validation Loss: 0.12299805134534836 \n",
            "At 49 epoch, Training Loss: 0.1233196273446083 \n",
            "At 49 epoch, Validation Loss: 0.11730453372001648 \n",
            "At 50 epoch, Training Loss: 0.12052032798528671 \n",
            "At 50 epoch, Validation Loss: 0.11387882381677628 \n",
            "At 51 epoch, Training Loss: 0.11754589229822159 \n",
            "At 51 epoch, Validation Loss: 0.10951481014490128 \n",
            "At 52 epoch, Training Loss: 0.11451410949230194 \n",
            "At 52 epoch, Validation Loss: 0.10481885075569153 \n",
            "At 53 epoch, Training Loss: 0.11067342758178711 \n",
            "At 53 epoch, Validation Loss: 0.10085252672433853 \n",
            "At 54 epoch, Training Loss: 0.1093223437666893 \n",
            "At 54 epoch, Validation Loss: 0.09721387922763824 \n",
            "At 55 epoch, Training Loss: 0.10694304406642914 \n",
            "At 55 epoch, Validation Loss: 0.09475192427635193 \n",
            "At 56 epoch, Training Loss: 0.10486874133348464 \n",
            "At 56 epoch, Validation Loss: 0.09247040003538132 \n",
            "At 57 epoch, Training Loss: 0.10281215608119965 \n",
            "At 57 epoch, Validation Loss: 0.08953119069337845 \n",
            "At 58 epoch, Training Loss: 0.10002569854259491 \n",
            "At 58 epoch, Validation Loss: 0.08684637397527695 \n",
            "At 59 epoch, Training Loss: 0.09689448773860931 \n",
            "At 59 epoch, Validation Loss: 0.08440426737070084 \n",
            "At 60 epoch, Training Loss: 0.09409773945808411 \n",
            "At 60 epoch, Validation Loss: 0.08300376683473587 \n",
            "At 61 epoch, Training Loss: 0.09200384318828583 \n",
            "At 61 epoch, Validation Loss: 0.08173719048500061 \n",
            "At 62 epoch, Training Loss: 0.09029025882482529 \n",
            "At 62 epoch, Validation Loss: 0.0798279270529747 \n",
            "At 63 epoch, Training Loss: 0.08977808803319931 \n",
            "At 63 epoch, Validation Loss: 0.0773562416434288 \n",
            "At 64 epoch, Training Loss: 0.08811307847499847 \n",
            "At 64 epoch, Validation Loss: 0.07538557052612305 \n",
            "At 65 epoch, Training Loss: 0.08463987708091736 \n",
            "At 65 epoch, Validation Loss: 0.07380370050668716 \n",
            "At 66 epoch, Training Loss: 0.0833955854177475 \n",
            "At 66 epoch, Validation Loss: 0.07273278385400772 \n",
            "At 67 epoch, Training Loss: 0.08241811692714691 \n",
            "At 67 epoch, Validation Loss: 0.07076945155858994 \n",
            "At 68 epoch, Training Loss: 0.07803113609552384 \n",
            "At 68 epoch, Validation Loss: 0.06898524612188339 \n",
            "At 69 epoch, Training Loss: 0.07894750833511352 \n",
            "At 69 epoch, Validation Loss: 0.0679684728384018 \n",
            "At 70 epoch, Training Loss: 0.07752318978309632 \n",
            "At 70 epoch, Validation Loss: 0.06668176501989365 \n",
            "At 71 epoch, Training Loss: 0.07483730763196945 \n",
            "At 71 epoch, Validation Loss: 0.06531782448291779 \n",
            "At 72 epoch, Training Loss: 0.07261419147253037 \n",
            "At 72 epoch, Validation Loss: 0.0632530152797699 \n",
            "At 73 epoch, Training Loss: 0.07139145433902741 \n",
            "At 73 epoch, Validation Loss: 0.06276287138462067 \n",
            "At 74 epoch, Training Loss: 0.0700569674372673 \n",
            "At 74 epoch, Validation Loss: 0.06209331750869751 \n",
            "At 75 epoch, Training Loss: 0.06878144890069962 \n",
            "At 75 epoch, Validation Loss: 0.0603412501513958 \n",
            "At 76 epoch, Training Loss: 0.06632560789585114 \n",
            "At 76 epoch, Validation Loss: 0.05890967324376106 \n",
            "At 77 epoch, Training Loss: 0.06632430702447892 \n",
            "At 77 epoch, Validation Loss: 0.05792390555143356 \n",
            "At 78 epoch, Training Loss: 0.06450913995504379 \n",
            "At 78 epoch, Validation Loss: 0.056859057396650314 \n",
            "At 79 epoch, Training Loss: 0.06374100744724273 \n",
            "At 79 epoch, Validation Loss: 0.056227415800094604 \n",
            "At 80 epoch, Training Loss: 0.061488992720842364 \n",
            "At 80 epoch, Validation Loss: 0.05476309731602669 \n",
            "At 81 epoch, Training Loss: 0.061538978666067126 \n",
            "At 81 epoch, Validation Loss: 0.05371583253145218 \n",
            "At 82 epoch, Training Loss: 0.059336303174495696 \n",
            "At 82 epoch, Validation Loss: 0.05361834913492203 \n",
            "At 83 epoch, Training Loss: 0.058891701698303225 \n",
            "At 83 epoch, Validation Loss: 0.05264640972018242 \n",
            "At 84 epoch, Training Loss: 0.05693196952342987 \n",
            "At 84 epoch, Validation Loss: 0.05190238729119301 \n",
            "At 85 epoch, Training Loss: 0.05750526413321495 \n",
            "At 85 epoch, Validation Loss: 0.051017045974731445 \n",
            "At 86 epoch, Training Loss: 0.05638527795672417 \n",
            "At 86 epoch, Validation Loss: 0.05006984621286392 \n",
            "At 87 epoch, Training Loss: 0.05386437401175499 \n",
            "At 87 epoch, Validation Loss: 0.0496637299656868 \n",
            "At 88 epoch, Training Loss: 0.05581994131207466 \n",
            "At 88 epoch, Validation Loss: 0.04896783083677292 \n",
            "At 89 epoch, Training Loss: 0.053086640685796736 \n",
            "At 89 epoch, Validation Loss: 0.048334408551454544 \n",
            "At 90 epoch, Training Loss: 0.05149091705679894 \n",
            "At 90 epoch, Validation Loss: 0.04738106206059456 \n",
            "At 91 epoch, Training Loss: 0.05019647926092148 \n",
            "At 91 epoch, Validation Loss: 0.04694943130016327 \n",
            "At 92 epoch, Training Loss: 0.05042569041252136 \n",
            "At 92 epoch, Validation Loss: 0.04621211066842079 \n",
            "At 93 epoch, Training Loss: 0.04921192601323128 \n",
            "At 93 epoch, Validation Loss: 0.04612172394990921 \n",
            "At 94 epoch, Training Loss: 0.04852786138653755 \n",
            "At 94 epoch, Validation Loss: 0.04531604424118996 \n",
            "At 95 epoch, Training Loss: 0.04806237146258354 \n",
            "At 95 epoch, Validation Loss: 0.04480932280421257 \n",
            "At 96 epoch, Training Loss: 0.047171156853437424 \n",
            "At 96 epoch, Validation Loss: 0.04339364543557167 \n",
            "At 97 epoch, Training Loss: 0.04592791795730591 \n",
            "At 97 epoch, Validation Loss: 0.04275539144873619 \n",
            "At 98 epoch, Training Loss: 0.04547493383288383 \n",
            "At 98 epoch, Validation Loss: 0.04255634918808937 \n",
            "At 99 epoch, Training Loss: 0.045282486081123355 \n",
            "At 99 epoch, Validation Loss: 0.042135417461395264 \n",
            "At 100 epoch, Training Loss: 0.043329577147960666 \n",
            "At 100 epoch, Validation Loss: 0.04122433811426163 \n",
            "At 101 epoch, Training Loss: 0.04244953766465187 \n",
            "At 101 epoch, Validation Loss: 0.04110897704958916 \n",
            "At 102 epoch, Training Loss: 0.04310760423541069 \n",
            "At 102 epoch, Validation Loss: 0.040965091437101364 \n",
            "At 103 epoch, Training Loss: 0.04118824601173401 \n",
            "At 103 epoch, Validation Loss: 0.04037894308567047 \n",
            "At 104 epoch, Training Loss: 0.041192421317100526 \n",
            "At 104 epoch, Validation Loss: 0.03935784101486206 \n",
            "At 105 epoch, Training Loss: 0.0401176430284977 \n",
            "At 105 epoch, Validation Loss: 0.03937392309308052 \n",
            "At 106 epoch, Training Loss: 0.04034906774759293 \n",
            "At 106 epoch, Validation Loss: 0.03944339603185654 \n",
            "At 107 epoch, Training Loss: 0.03830648064613342 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs6gugiyDvl1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "7d4e683b-3d2e-4af9-c050-a4cda45d880b"
      },
      "source": [
        "outputs=[]\n",
        "targets=[]\n",
        "for data,target in test_loader:\n",
        "    data,target=data.cuda(),target.cuda()\n",
        "    output=model(data.float())\n",
        "    output=output.cpu()\n",
        "    output=list(np.where(output<0,0,1)[:])\n",
        "    target=list(target.cpu().numpy())\n",
        "    outputs+=output\n",
        "    targets+=target\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "ts=0\n",
        "for i in outputs:\n",
        "    ts+=i\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "outputs_of_model=np.stack(outputs,axis=0).squeeze().astype(dtype=int)\n",
        "targets_of_model=np.array(targets).astype(dtype=int)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "TN=np.array(targets_of_model[outputs_of_model==0]==0).sum()\n",
        "FP=np.array(targets_of_model[outputs_of_model==1]==0).sum()\n",
        "FN=np.array(targets_of_model[outputs_of_model==0]==1).sum()\n",
        "TP=np.array(targets_of_model[outputs_of_model==1]==1).sum()\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "print(\"True Positives: {}\".format(TP))\n",
        "print(\"True Negatives: {}\".format(TN))\n",
        "print(\"False Positives: {}\".format(FP))\n",
        "print(\"False Negatives: {}\".format(FN))\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "precision=TP/(TP+FP)\n",
        "recall=TP/(TP+FN)\n",
        "print(precision)\n",
        "print(recall)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "F1=2*precision*recall/(precision+recall)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "print(F1)\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True Positives: 41\n",
            "True Negatives: 28034\n",
            "False Positives: 404\n",
            "False Negatives: 2\n",
            "0.09213483146067415\n",
            "0.9534883720930233\n",
            "0.1680327868852459\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Kx33KR3IYVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}