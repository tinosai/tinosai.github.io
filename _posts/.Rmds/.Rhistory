#calculate classwise means
mu_k <- c()
for (i in 1:length(prop)){
mu_k <- cbind(mu_k,colMeans(data[data[,3]==i,c(1,2)]))
}
#calculate covariance matrix
covar_matrix <- matrix(0,ncol=2,nrow=2)
for (i in 1:length(prop)){
covar_matrix <- covar_matrix + cov(data[data[,3]==i,c(1,2)])/prop[i]
}
#rescale covariance matrix
covar_matrix <- covar_matrix/(dim(data)[1]-length(prop))
#strip data from useless columns
data <- as.matrix(data[,c(1,2)])
colnames(data) <- NULL
rownames(data) <- NULL
first_column <- seq(min(data[,1]),max(data[,1]),length.out=splits)
second_column <- seq(min(data[,2]),max(data[,2]),length.out=splits)
combination <- as.matrix(expand.grid(first_column,first_column))
colnames(combination) <- NULL
#infer on the combinations
discriminant <- c()
sigma_m1 <- solve(covar_matrix)
for (i in 1:length(prop)){
temp_d <- combination %*% sigma_m1 %*% matrix(mu_k[,i])
temp_d <- temp_d - c(0.5*t(matrix(mu_k[,i])) %*% sigma_m1 %*% matrix(mu_k[,i]))
temp_d <- temp_d  + log(prop[i])
discriminant <- cbind(discriminant,temp_d)
}
#take the max over the discriminant fucnction
max_discriminant <- apply(discriminant,MARGIN=1,FUN=which.max)
combination <- data.frame(cbind(combination,max_discriminant))
colnames(combination) <- c("Y_1","Y_2","ClassB")
return(combination)
}
contour_calculation_qda <- function(data,splits=100){
#calculate proportion (prior)
prop <- table(as.numeric(data[,3]))
rownames(prop) <- NULL
prop <- prop/sum(prop)
#calculate classwise means
mu_k <- c()
for (i in 1:length(prop)){
mu_k <- cbind(mu_k,colMeans(data[data[,3]==i,c(1,2)]))
}
#calculate covariance matrices
covar_matrix <- array(0,dim=c(2,2,length(prop)))
for (i in 1:length(prop)){
covar_matrix[,,i] <- cov(data[data[,3]==i,c(1,2)])
}
#strip data from useless columns
data <- as.matrix(data[,c(1,2)])
colnames(data) <- NULL
rownames(data) <- NULL
first_column <- seq(min(data[,1]),max(data[,1]),length.out=splits)
second_column <- seq(min(data[,2]),max(data[,2]),length.out=splits)
combination <- as.matrix(expand.grid(first_column,first_column))
colnames(combination) <- NULL
#infer on the combinations, calculating discriminants
discriminant <- c()
for (i in 1:length(prop)){
s_1 <- solve(covar_matrix[,,i])
#adding the quadratic term below. Using colSums to prevent memory
#overflow
temp_d <- -0.5 * matrix(colSums(t(combination) * (s_1 %*% t(combination))))
temp_d <- temp_d + c(combination %*% s_1 %*% matrix(mu_k[,i]))
temp_d <- temp_d - c(0.5*t(matrix(mu_k[,i])) %*% s_1 %*% matrix(mu_k[,i]))
temp_d <- temp_d  + log(prop[i]) - 0.5*log(det(covar_matrix[,,i]))
discriminant <- cbind(discriminant,temp_d)
}
#take the max over the discriminant fucnction
max_discriminant <- apply(discriminant,MARGIN=1,FUN=which.max)
combination <- data.frame(cbind(combination,max_discriminant))
colnames(combination) <- c("Y_1","Y_2","ClassB")
return(combination)
}
library(MASS) #import for LDA, QDA
library(patchwork) #import for plots stacking
set.seed(123) #set seed for repeatability.
n <- 300 #set the number of samples
p <- c(1/3,1/3,1/3) #set the prior probability of the classes
mus <- list(mu1=c(-1,1), mu2=c(1,1), mu3=c(3,1)) #define list of means
sigma <- matrix(c(1,0.7,0.7,1),ncol=2) #define covariance matrix
sigmas <- list(sigma_1 = sigma, sigma_2 = sigma, sigma_3 = sigma)
output_mixture <- sim_bvn_mixture(n,p,mus,sigmas) #gaussian mixture
#define plotting function to be reutilized
plotter <- function(dataset,title,col_name,type="none"){
g <- ggplot() + geom_point(data=dataset,aes_string(x="y_1",y="y_2",color=col_name),alpha=0.5) +
coord_equal() + xlab("Y1") + ylab("Y2") + ggtitle(title) + theme(plot.title = element_text(size=12, hjust=0.5))
if (type == "lda"){
contours <- contour_calculation_lda(dataset,splits=1000)
g <- g + geom_contour(data=contours, aes(x=Y_1,y=Y_2,z=ClassB),alpha=0.5,color="black")
} else if (type=="qda"){
contours <- contour_calculation_qda(dataset,splits=1000)
g <- g + geom_contour(data=contours, aes(x=Y_1,y=Y_2,z=ClassB),alpha=0.5,color="black")
}
return(g)
}
lda_norm <- lda(component ~ y_1 + y_2 , data=output_mixture) #train classifier
output_mixture$predictions <- predict(lda_norm)$class #predict
#plot the distribution
g1 <- plotter(output_mixture,"BGMD, ground truth","component")
g2 <- plotter(output_mixture,"BGMD, predicted","predictions",type="lda")
g1 + g2 #stack plots horizontally
for (i in 1:100){
lda_norm <- lda(component ~ y_1 + y_2 , data=output_mixture) #train classifier
output_mixture$predictions <- predict(lda_norm)$class #predict
errors <- c(errors,1-mean(output_mixture$predictions == output_mixture$component)) #error rate
}
#evaluate errors
#repeat experiment 100 times
errors <- c()
for (i in 1:100){
lda_norm <- lda(component ~ y_1 + y_2 , data=output_mixture) #train classifier
output_mixture$predictions <- predict(lda_norm)$class #predict
errors <- c(errors,1-mean(output_mixture$predictions == output_mixture$component)) #error rate
}
boxplot(errors)
errors
for (i in 1:100){
output_mixture <- sim_bvn_mixture(n,p,mus,sigmas) #gaussian mixture
lda_norm <- lda(component ~ y_1 + y_2 , data=output_mixture) #train classifier
output_mixture$predictions <- predict(lda_norm)$class #predict
errors <- c(errors,1-mean(output_mixture$predictions == output_mixture$component)) #error rate
}
#evaluate errors
#repeat experiment 100 times
errors <- c()
for (i in 1:100){
output_mixture <- sim_bvn_mixture(n,p,mus,sigmas) #gaussian mixture
lda_norm <- lda(component ~ y_1 + y_2 , data=output_mixture) #train classifier
output_mixture$predictions <- predict(lda_norm)$class #predict
errors <- c(errors,1-mean(output_mixture$predictions == output_mixture$component)) #error rate
}
boxplot(errors)
#evaluate errors
#repeat experiment 100 times
errors <- c()
for (i in 1:1000){
output_mixture <- sim_bvn_mixture(n,p,mus,sigmas) #gaussian mixture
lda_norm <- lda(component ~ y_1 + y_2 , data=output_mixture) #train classifier
output_mixture$predictions <- predict(lda_norm)$class #predict
errors <- c(errors,1-mean(output_mixture$predictions == output_mixture$component)) #error rate
}
boxplot(errors)
errors <- c()
for (i in 1:1000){
output_mixture_hetero <- sim_bvn_mixture(n,p,mus,sigmas)
qda_norm_hetero <- lda(component ~ y_1 + y_2 , data=output_mixture_hetero)
output_mixture_hetero$predictions_lda <- predict(qda_norm_hetero)$class
errors <- c(errors,1-mean(output_mixture_hetero$predictions_qda == output_mixture_hetero$component)) #error rate
}
boxplot(errors)
errors
errors <- c()
for (i in 1:1000){
output_mixture_hetero <- sim_bvn_mixture(n,p,mus,sigmas)
qda_norm_hetero <- qda(component ~ y_1 + y_2 , data=output_mixture_hetero)
output_mixture_hetero$predictions_lda <- predict(qda_norm_hetero)$class
errors <- c(errors,1-mean(output_mixture_hetero$predictions_qda == output_mixture_hetero$component)) #error rate
}
errors
output_mixture_hetero
errors <- c()
for (i in 1:1000){
output_mixture_hetero <- sim_bvn_mixture(n,p,mus,sigmas)
qda_norm_hetero <- qda(component ~ y_1 + y_2 , data=output_mixture_hetero)
output_mixture_hetero$predictions_qda <- predict(qda_norm_hetero)$class
errors <- c(errors,1-mean(output_mixture_hetero$predictions_qda == output_mixture_hetero$component)) #error rate
}
boxplot(errors)
errors
boxplot(errors)
errors <- c()
for (i in 1:1000){
output_t <- data.frame( y_1 = classes[,1], y_2 = classes[,2], component = classes[,3])
output_t$component <- as.factor(output_t$component)
qda_t <- qda(component ~ y_1 + y_2 , data=output_t)
output_t$predictions_qda <- predict(qda_t)$class
errors <- c(errors,1-mean(output_t$predictions_qda == output_t$component)) #error rate
}
library(mvtnorm)
#generate a Multivariate T Mixture Distribution
n <- 200
class_t1<- cbind(rmvt(n=n,df = 1,sigma = sigmas[[1]])+matrix(rep(mus[[1]],n),ncol=2,byrow=T),1)
class_t2<- cbind(rmvt(n=n,df = 2,sigma = sigmas[[2]])+matrix(rep(mus[[2]],n),ncol=2,byrow=T),2)
class_t3<- cbind(rmvt(n=n,df = 3,sigma = sigmas[[3]])+matrix(rep(mus[[3]],n),ncol=2,byrow=T),3)
#generate data frame
classes <- rbind(class_t1,class_t2,class_t3)
output_t <- data.frame( y_1 = classes[,1], y_2 = classes[,2], component = classes[,3])
output_t$component <- as.factor(output_t$component)
#fit model on the distribution
qda_t <- qda(component ~ y_1 + y_2 , data=output_t)
output_t$predictions_qda <- predict(qda_t)$class
#plot the distribution
g6 <- plotter(output_t,"BTMD, ground truth","component")
g7 <- plotter(output_t,"BTMD, predicted","predictions_qda",type="qda")
g6 + g7
errors <- c()
for (i in 1:1000){
output_t <- data.frame( y_1 = classes[,1], y_2 = classes[,2], component = classes[,3])
output_t$component <- as.factor(output_t$component)
qda_t <- qda(component ~ y_1 + y_2 , data=output_t)
output_t$predictions_qda <- predict(qda_t)$class
errors <- c(errors,1-mean(output_t$predictions_qda == output_t$component)) #error rate
}
boxplot(errors)
errors
1-mean(output_t$predictions_qda == output_t$component) #error rate
boxplot(errors)
errors <- c()
for (i in 1:1000){
class_t1<- cbind(rmvt(n=n,df = 1,sigma = sigmas[[1]])+matrix(rep(mus[[1]],n),ncol=2,byrow=T),1)
class_t2<- cbind(rmvt(n=n,df = 2,sigma = sigmas[[2]])+matrix(rep(mus[[2]],n),ncol=2,byrow=T),2)
class_t3<- cbind(rmvt(n=n,df = 3,sigma = sigmas[[3]])+matrix(rep(mus[[3]],n),ncol=2,byrow=T),3)
output_t <- data.frame( y_1 = classes[,1], y_2 = classes[,2], component = classes[,3])
output_t$component <- as.factor(output_t$component)
qda_t <- qda(component ~ y_1 + y_2 , data=output_t)
output_t$predictions_qda <- predict(qda_t)$class
errors <- c(errors,1-mean(output_t$predictions_qda == output_t$component)) #error rate
}
boxplot(errors)
errors
boxplot(errors)
errors <- c()
for (i in 1:1000){
class_t1<- cbind(rmvt(n=n,df = 1,sigma = sigmas[[1]])+matrix(rep(mus[[1]],n),ncol=2,byrow=T),1)
class_t2<- cbind(rmvt(n=n,df = 2,sigma = sigmas[[2]])+matrix(rep(mus[[2]],n),ncol=2,byrow=T),2)
class_t3<- cbind(rmvt(n=n,df = 3,sigma = sigmas[[3]])+matrix(rep(mus[[3]],n),ncol=2,byrow=T),3)
classes <- rbind(class_t1,class_t2,class_t3)
output_t <- data.frame( y_1 = classes[,1], y_2 = classes[,2], component = classes[,3])
output_t$component <- as.factor(output_t$component)
qda_t <- qda(component ~ y_1 + y_2 , data=output_t)
output_t$predictions_qda <- predict(qda_t)$class
errors <- c(errors,1-mean(output_t$predictions_qda == output_t$component)) #error rate
}
boxplot(errors)
errors
boxplot(errors)
ggplot() + geom_boxplot(errors)
ggplot() + geom_boxplot(aes(errors))
ggplot() + geom_boxplot(aes(errors)) + coord_flip()
errors_4
errors_4 <- c()
for (i in 1:1000){
class_t1<- cbind(rmvt(n=n,df = 1,sigma = sigmas[[1]])+matrix(rep(mus[[1]],n),ncol=2,byrow=T),1)
class_t2<- cbind(rmvt(n=n,df = 2,sigma = sigmas[[2]])+matrix(rep(mus[[2]],n),ncol=2,byrow=T),2)
class_t3<- cbind(rmvt(n=n,df = 3,sigma = sigmas[[3]])+matrix(rep(mus[[3]],n),ncol=2,byrow=T),3)
classes <- rbind(class_t1,class_t2,class_t3)
output_t <- data.frame( y_1 = classes[,1], y_2 = classes[,2], component = classes[,3])
output_t$component <- as.factor(output_t$component)
qda_t <- qda(component ~ y_1 + y_2 , data=output_t)
output_t$predictions_qda <- predict(qda_t)$class
errors_4 <- c(errors_4,1-mean(output_t$predictions_qda == output_t$component)) #error rate
}
errors_4
errors_4
ggplot() + geom_qqplot(errors_4) + coord_flip()
ggplot() + geom_boxplot(errors_4) + coord_flip()
ggplot() + geom_boxplot(aes(errors_4)) + coord_flip()
ggplot() + geom_boxplot(aes(errors_4)) + coord_flip() + ylim(0,1)
ggplot() + geom_boxplot(aes(errors_4)) + coord_flip() + xlim(0,1)
#dataframe which groups all the errors together
df_errors <- data.frame(lda_homo = error_1, lda_hetero = error_2, qda_hetero = error_3
, qda_t = error_4)
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
source("../q1/sl_assessment_2_q1_02165798.R")
#import the library for LDA and QDA
contour_calculation_lda <- function(data,splits=100){
#calculate proportion (prior)
prop <- table(as.numeric(data[,3]))
rownames(prop) <- NULL
prop <- prop/sum(prop)
#calculate classwise means
mu_k <- c()
for (i in 1:length(prop)){
mu_k <- cbind(mu_k,colMeans(data[data[,3]==i,c(1,2)]))
}
#calculate covariance matrix
covar_matrix <- matrix(0,ncol=2,nrow=2)
for (i in 1:length(prop)){
covar_matrix <- covar_matrix + cov(data[data[,3]==i,c(1,2)])/prop[i]
}
#rescale covariance matrix
covar_matrix <- covar_matrix/(dim(data)[1]-length(prop))
#strip data from useless columns
data <- as.matrix(data[,c(1,2)])
colnames(data) <- NULL
rownames(data) <- NULL
first_column <- seq(min(data[,1]),max(data[,1]),length.out=splits)
second_column <- seq(min(data[,2]),max(data[,2]),length.out=splits)
combination <- as.matrix(expand.grid(first_column,first_column))
colnames(combination) <- NULL
#infer on the combinations
discriminant <- c()
sigma_m1 <- solve(covar_matrix)
for (i in 1:length(prop)){
temp_d <- combination %*% sigma_m1 %*% matrix(mu_k[,i])
temp_d <- temp_d - c(0.5*t(matrix(mu_k[,i])) %*% sigma_m1 %*% matrix(mu_k[,i]))
temp_d <- temp_d  + log(prop[i])
discriminant <- cbind(discriminant,temp_d)
}
#take the max over the discriminant fucnction
max_discriminant <- apply(discriminant,MARGIN=1,FUN=which.max)
combination <- data.frame(cbind(combination,max_discriminant))
colnames(combination) <- c("Y_1","Y_2","ClassB")
return(combination)
}
contour_calculation_qda <- function(data,splits=100){
#calculate proportion (prior)
prop <- table(as.numeric(data[,3]))
rownames(prop) <- NULL
prop <- prop/sum(prop)
#calculate classwise means
mu_k <- c()
for (i in 1:length(prop)){
mu_k <- cbind(mu_k,colMeans(data[data[,3]==i,c(1,2)]))
}
#calculate covariance matrices
covar_matrix <- array(0,dim=c(2,2,length(prop)))
for (i in 1:length(prop)){
covar_matrix[,,i] <- cov(data[data[,3]==i,c(1,2)])
}
#strip data from useless columns
data <- as.matrix(data[,c(1,2)])
colnames(data) <- NULL
rownames(data) <- NULL
first_column <- seq(min(data[,1]),max(data[,1]),length.out=splits)
second_column <- seq(min(data[,2]),max(data[,2]),length.out=splits)
combination <- as.matrix(expand.grid(first_column,first_column))
colnames(combination) <- NULL
#infer on the combinations, calculating discriminants
discriminant <- c()
for (i in 1:length(prop)){
s_1 <- solve(covar_matrix[,,i])
#adding the quadratic term below. Using colSums to prevent memory
#overflow
temp_d <- -0.5 * matrix(colSums(t(combination) * (s_1 %*% t(combination))))
temp_d <- temp_d + c(combination %*% s_1 %*% matrix(mu_k[,i]))
temp_d <- temp_d - c(0.5*t(matrix(mu_k[,i])) %*% s_1 %*% matrix(mu_k[,i]))
temp_d <- temp_d  + log(prop[i]) - 0.5*log(det(covar_matrix[,,i]))
discriminant <- cbind(discriminant,temp_d)
}
#take the max over the discriminant fucnction
max_discriminant <- apply(discriminant,MARGIN=1,FUN=which.max)
combination <- data.frame(cbind(combination,max_discriminant))
colnames(combination) <- c("Y_1","Y_2","ClassB")
return(combination)
}
library(MASS) #import for LDA, QDA
library(patchwork) #import for plots stacking
set.seed(123) #set seed for repeatability.
n <- 300 #set the number of samples
p <- c(1/3,1/3,1/3) #set the prior probability of the classes
mus <- list(mu1=c(-1,1), mu2=c(1,1), mu3=c(3,1)) #define list of means
sigma <- matrix(c(1,0.7,0.7,1),ncol=2) #define covariance matrix
sigmas <- list(sigma_1 = sigma, sigma_2 = sigma, sigma_3 = sigma)
output_mixture <- sim_bvn_mixture(n,p,mus,sigmas) #gaussian mixture
#define plotting function to be reutilized
plotter <- function(dataset,title,col_name,type="none"){
g <- ggplot() + geom_point(data=dataset,aes_string(x="y_1",y="y_2",color=col_name),alpha=0.5) +
coord_equal() + xlab("Y1") + ylab("Y2") + ggtitle(title) + theme(plot.title = element_text(size=12, hjust=0.5))
if (type == "lda"){
contours <- contour_calculation_lda(dataset,splits=1000)
g <- g + geom_contour(data=contours, aes(x=Y_1,y=Y_2,z=ClassB),alpha=0.5,color="black")
} else if (type=="qda"){
contours <- contour_calculation_qda(dataset,splits=1000)
g <- g + geom_contour(data=contours, aes(x=Y_1,y=Y_2,z=ClassB),alpha=0.5,color="black")
}
return(g)
}
lda_norm <- lda(component ~ y_1 + y_2 , data=output_mixture) #train classifier
output_mixture$predictions <- predict(lda_norm)$class #predict
#plot the distribution
g1 <- plotter(output_mixture,"BGMD, ground truth","component")
g2 <- plotter(output_mixture,"BGMD, predicted","predictions",type="lda")
g1 + g2 #stack plots horizontally
#evaluate errors
#repeat experiment 100 times
errors_1 <- c()
for (i in 1:1000){
output_mixture <- sim_bvn_mixture(n,p,mus,sigmas) #gaussian mixture
lda_norm <- lda(component ~ y_1 + y_2 , data=output_mixture) #train classifier
output_mixture$predictions <- predict(lda_norm)$class #predict
errors_1 <- c(errors_1,1-mean(output_mixture$predictions == output_mixture$component)) #error rate
}
boxplot(errors_1)
sigmas <- list(sigma_1 = matrix(c(0.8,1,1,2),ncol=2),
sigma_2 = matrix(c(3,0,0,8),ncol=2),
sigma_3 = matrix(c(20,1,1,2),ncol=2))
output_mixture_hetero <- sim_bvn_mixture(n,p,mus,sigmas)
lda_norm_hetero <- lda(component ~ y_1 + y_2 , data=output_mixture_hetero)
output_mixture_hetero$predictions_lda <- predict(lda_norm_hetero)$class #generate sample
#plot the distribution
g3 <- plotter(output_mixture_hetero,"BGMD, ground truth","component")
g4 <- plotter(output_mixture_hetero,"BGMD, predicted","predictions_lda",type="lda")
g3 + g4 #stack plots horizontally
#
1-mean(output_mixture_hetero$predictions_lda == output_mixture_hetero$component) #error rate
#evaluate errors
#repeat experiment 100 times
errors_2 <- c()
for (i in 1:1000){
output_mixture_hetero <- sim_bvn_mixture(n,p,mus,sigmas)
lda_norm_hetero <- lda(component ~ y_1 + y_2 , data=output_mixture_hetero)
output_mixture_hetero$predictions_lda <- predict(lda_norm_hetero)$class
errors_2 <- c(errors_2,1-mean(output_mixture_hetero$predictions_lda == output_mixture_hetero$component)) #error rate) #error rate
}
boxplot(errors_2)
qda_norm_hetero <- qda(component ~ y_1 + y_2 , data=output_mixture_hetero)
output_mixture_hetero$predictions_qda <- predict(qda_norm_hetero)$class
g5 <- plotter(output_mixture_hetero,"BGMD, predicted","predictions_qda","qda")
g3 + g5 #stack plots horizontally
errors_3 <- c()
for (i in 1:1000){
output_mixture_hetero <- sim_bvn_mixture(n,p,mus,sigmas)
qda_norm_hetero <- qda(component ~ y_1 + y_2 , data=output_mixture_hetero)
output_mixture_hetero$predictions_qda <- predict(qda_norm_hetero)$class
errors_3 <- c(errors_3,1-mean(output_mixture_hetero$predictions_qda == output_mixture_hetero$component)) #error rate
}
boxplot(errors_3)
library(mvtnorm)
#generate a Multivariate T Mixture Distribution
n <- 200
class_t1<- cbind(rmvt(n=n,df = 1,sigma = sigmas[[1]])+matrix(rep(mus[[1]],n),ncol=2,byrow=T),1)
class_t2<- cbind(rmvt(n=n,df = 2,sigma = sigmas[[2]])+matrix(rep(mus[[2]],n),ncol=2,byrow=T),2)
class_t3<- cbind(rmvt(n=n,df = 3,sigma = sigmas[[3]])+matrix(rep(mus[[3]],n),ncol=2,byrow=T),3)
#generate data frame
classes <- rbind(class_t1,class_t2,class_t3)
output_t <- data.frame( y_1 = classes[,1], y_2 = classes[,2], component = classes[,3])
output_t$component <- as.factor(output_t$component)
#fit model on the distribution
qda_t <- qda(component ~ y_1 + y_2 , data=output_t)
output_t$predictions_qda <- predict(qda_t)$class
#plot the distribution
g6 <- plotter(output_t,"BTMD, ground truth","component")
g7 <- plotter(output_t,"BTMD, predicted","predictions_qda",type="qda")
g6 + g7
#dataframe which groups all the errors together
df_errors <- data.frame(lda_homo = error_1, lda_hetero = error_2, qda_hetero = error_3
, qda_t = error_4)
errors_4 <- c()
for (i in 1:1000){
class_t1<- cbind(rmvt(n=n,df = 1,sigma = sigmas[[1]])+matrix(rep(mus[[1]],n),ncol=2,byrow=T),1)
class_t2<- cbind(rmvt(n=n,df = 2,sigma = sigmas[[2]])+matrix(rep(mus[[2]],n),ncol=2,byrow=T),2)
class_t3<- cbind(rmvt(n=n,df = 3,sigma = sigmas[[3]])+matrix(rep(mus[[3]],n),ncol=2,byrow=T),3)
classes <- rbind(class_t1,class_t2,class_t3)
output_t <- data.frame( y_1 = classes[,1], y_2 = classes[,2], component = classes[,3])
output_t$component <- as.factor(output_t$component)
qda_t <- qda(component ~ y_1 + y_2 , data=output_t)
output_t$predictions_qda <- predict(qda_t)$class
errors_4 <- c(errors_4,1-mean(output_t$predictions_qda == output_t$component)) #error rate
}
#dataframe which groups all the errors together
df_errors <- data.frame(lda_homo = error_1, lda_hetero = error_2, qda_hetero = error_3
, qda_t = error_4)
#dataframe which groups all the errors together
df_errors <- data.frame(lda_homo = errors_1, lda_hetero = errors_2,
qda_hetero = errors_3, qda_t = errors_4)
df_errors
ggplot() + geom_boxplot(aes(y=errors_1))
ggplot() + geom_boxplot(aes(y=errors_1)) + geom_boxplot(aes(y=errors_2))
ggplot() + geom_boxplot(aes(x="errors_1",y=errors_1))
ggplot(data=df_errors) + geom_boxplot(aes(y=lda_homo))
ggplot(data=df_errors) + geom_boxplot(aes(y=lda_homo)) + geom_boxplot(aes(y=lda_hetero))
df2<-melt(df_errors,id.var=c("lda_homo","lda_hetero","qda_hetero","qda_t"))
#dataframe which groups all the errors together
df_errors <- data.frame(label="lda_homo",error=errors_1)
df_errors
df_errors <- rbind(df_errors, data.frame(label="qda_t",error=errors_4))
#dataframe which groups all the errors together
df_errors <- data.frame(label="lda_homo",error=errors_1)
df_errors <- rbind(df_errors, data.frame(label="lda_hetero",error=errors_2))
df_errors <- rbind(df_errors, data.frame(label="qda_hetero",error=errors_3))
df_errors <- rbind(df_errors, data.frame(label="qda_t",error=errors_4))
ggplot(data=df_errors,aes(y=error,x=label)) + geom_boxplot()
#dataframe which groups all the errors together
df_errors <- data.frame(label="1) LDA Homoscedastic",error=errors_1)
df_errors <- rbind(df_errors, data.frame(label="2) LDA Heteroscedastic",error=errors_2))
df_errors <- rbind(df_errors, data.frame(label="3) QDA Heteroscedastic",error=errors_3))
df_errors <- rbind(df_errors, data.frame(label="4) QDA T-Distribution",error=errors_4))
ggplot(data=df_errors,aes(y=error,x=label)) + geom_boxplot()
ggplot(data=df_errors,aes(y=error,x=label,color=label)) + geom_boxplot()
ggplot(data=df_errors,aes(y=error,x=label,fill=label)) + geom_boxplot()
ggplot(data=df_errors,aes(y=error,x=label,fill=label)) + geom_boxplot() +  theme(legend.position="none")
+ ylab("Error Rate")
ggplot(data=df_errors,aes(y=error,x=label,fill=label)) + geom_boxplot() +  theme(legend.position="none") +
ylab("Error Rate")
ggplot(data=df_errors,aes(y=error,x=label,fill=label)) + geom_boxplot() +  theme(legend.position="none") +
ylab("Error Rate") + xlab("Testing Set")
install,.packages("boot")
install.packages("boot")
library(boot)
library(ISLR2)
install.packages("ISLR2")
library(ISLR2)
attach(Portfolio)
Portfolio
ests_fn <- function(dataset,indices){
train <- sample(390,10)
train <- sample(390,10,replace=TRUE)
train
n
reticulate::repl_python()
setwd("~/blog/tinosai.github.io/_posts/.Rmds")
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_knit$set(base.dir = "/Users/fortunatonucera/blog/tinosai.github.io/", base.url = "/")
#knitr::opts_chunk$set(fig.path = "images/2022-09-19/")
library(reticulate)
reticulate::use_condaenv("base")
reticulate::repl_python()
